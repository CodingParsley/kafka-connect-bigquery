security.protocol=SASL_SSL
sasl.mechanism=PLAIN
request.timeout.ms=20000
#sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username\="scraper" password\="scraper-secret";
# bootstrap.servers=pkc-profound-crab.gcp.priv.cpdev.cloud:9092
retry.backoff.ms=500
ssl.endpoint.identification.algorithm=https
bootstrap.servers=pkc-42n01.us-west-2.aws.devel.cpdev.cloud:9092

sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username\="7YZ3LUXXUDEEWTJX" password\="BW4aNjwoOtY8L6BcoPTv25NKDxioJ0+/4mml751org9fKbihtPPO7zCd/Y2hq4SB";


consumer.ssl.endpoint.identification.algorithm=https
consumer.sasl.mechanism=PLAIN
consumer.request.timeout.ms=20000
consumer.retry.backoff.ms=500
consumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username\="7YZ3LUXXUDEEWTJX" password\="BW4aNjwoOtY8L6BcoPTv25NKDxioJ0+/4mml751org9fKbihtPPO7zCd/Y2hq4SB";
consumer.security.protocol=SASL_SSL
â€‹
producer.ssl.endpoint.identification.algorithm=https
producer.sasl.mechanism=PLAIN
producer.request.timeout.ms=20000
producer.retry.backoff.ms=500
producer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username\="7YZ3LUXXUDEEWTJX" password\="BW4aNjwoOtY8L6BcoPTv25NKDxioJ0+/4mml751org9fKbihtPPO7zCd/Y2hq4SB";
producer.security.protocol=SASL_SSL

# key.converter=org.apache.kafka.connect.json.JsonConverter
# key.converter.schemas.enable=false
# value.converter=org.apache.kafka.connect.json.JsonConverter
# value.converter.schemas.enable=false

# offset.storage.file.filename=/tmp/connect.offsets

key.converter=org.apache.kafka.connect.storage.StringConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=false
value.converter.schemas.enable=false

internal.key.converter=org.apache.kafka.connect.json.JsonConverter
internal.value.converter=org.apache.kafka.connect.json.JsonConverter
internal.key.converter.schemas.enable=false
internal.value.converter.schemas.enable=false

offset.storage.file.filename=/tmp/connect.offsets
offset.flush.interval.ms=10000


group.id=connect-cluster
#
# # Kafka topic where connector configuration will be persisted. You should create this topic with a
# # single partition and high replication factor (e.g. 3)
# config.storage.topic=metrics.json.kafka
#
# # Kafka topic where connector offset data will be persisted. You should create this topic with many
# # partitions (e.g. 25) and high replication factor (e.g. 3)
# offset.storage.topic=metrics.json.kafka
#
# # Kafka topic where connector status data will be persisted. You should create this topic with many
# # partitions (e.g. 25) and high replication factor (e.g. 3)
# status.storage.topic=metrics.json.kafka
#
#
# # Flush much faster than normal, which is useful for testing/debugging
# offset.flush.interval.ms=10000


plugin.path=/Users/nayachen/Desktop/kafka-connect-bigquery/bin/jar,share/java,~/confluent-5.2.2/share/confluent-hub-components